{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2021년도 코드로 TFT 실행해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kijeong\\anaconda3\\envs\\torch_book\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import Module\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from pytorch_forecasting.data import (\n",
    "    TimeSeriesDataSet,\n",
    "    GroupNormalizer\n",
    ")\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor\n",
    ")\n",
    "from pytorch_forecasting.metrics import SMAPE\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed random seed\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(42) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = {\n",
    "    0: [19, 20, 21, 49, 50, 51],\n",
    "    1: [1, 5, 9, 34],\n",
    "    2: [4, 10, 11, 12, 28, 29, 30, 36, 40, 41, 42, 59, 60],\n",
    "    3: [2, 3, 6, 7, 8, 13, 14, 15, 16, 17, 18, 22, 23, 24, 25, 26, 27, 31, 32, 33, 35, 37, 38, 39, 43, 44, 45, 46, 47, 48, 52, 53, 54, 55, 56, 57, 58],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 66\n",
    "batch_size = 128\n",
    "seed = 42\n",
    "encoder_length_in_weeks = 5\n",
    "\n",
    "params = {\n",
    "    'gradient_clip_val': 0.9658579636307634,\n",
    "    'hidden_size': 20,\n",
    "    'dropout': 0.19610151695402608,\n",
    "    'hidden_continuous_size': 10,\n",
    "    'attention_head_size': 4,\n",
    "    'learning_rate': 0.08\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate determined by a cv run with train data less 1 trailing week as validation \n",
    "lrs = [0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
    "       0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
    "       0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
    "       0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
    "       0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
    "       0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
    "       0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
    "       0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
    "       0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
    "       0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
    "       0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
    "       0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
    "       0.05099279397234306 , 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
    "       0.005099279397234306, 0.005099279397234306, 0.005099279397234306, 0.005099279397234306,\n",
    "       0.005099279397234306, 0.005099279397234306, 0.005099279397234306, 0.005099279397234306,\n",
    "       0.005099279397234306, 0.0005099279397234307, 0.0005099279397234307, 0.0005099279397234307,\n",
    "       0.0005099279397234307, 0.0005099279397234307, 0.0005099279397234307]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "sample_submission = pd.read_csv('../data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy\n",
    "train_df = train.copy()\n",
    "test_df = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the unnecessary columns for analysis\n",
    "train_df.drop(columns=['num_date_time', '일조(hr)', '일사(MJ/m2)'], inplace=True)\n",
    "test_df.drop(columns=['num_date_time'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "train_df.columns = ['building_num', 'date', 'temperature', 'precipitation', 'windspeed', 'humidity', 'power_consumption']\n",
    "test_df.columns = ['building_num', 'date', 'temperature', 'precipitation', 'windspeed', 'humidity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the missing values of precipitation\n",
    "train_df['precipitation'].fillna(0, inplace=True) # 강수량은 0으로 채움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the missing values of wind speed and humidity\n",
    "def fill_missing_with_avg(df, columns):\n",
    "    for i in range(len(df)):\n",
    "        if pd.isna(df.loc[i, columns]):\n",
    "            \n",
    "            prev_value_sum = df.loc[i-4:i-1, columns].sum()\n",
    "            next_value_sum = df.loc[i+1:i+4, columns].sum()\n",
    "            avg_value = (prev_value_sum + next_value_sum) / 8\n",
    "\n",
    "            df.loc[i, columns] = avg_value\n",
    "\n",
    "fill_missing_with_avg(train_df, 'windspeed')\n",
    "fill_missing_with_avg(train_df, 'humidity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the type of date from object to datetime\n",
    "train_df['date'] = pd.to_datetime(train_df['date'], format='%Y%m%d %H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decomposition of time elements\n",
    "train_df['month'] = train_df.date.dt.month\n",
    "train_df['day'] = train_df.date.dt.day\n",
    "train_df['weekday'] = train_df.date.dt.weekday\n",
    "train_df['hour'] = train_df.date.dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add time periodic columns\n",
    "day_periodic = (train_df['day'] - 1) / 30 # 1부터 시작하므로 1을 빼줌\n",
    "weekday_periodic = train_df['weekday'] / 6\n",
    "hour_periodic = train_df['hour'] / 23\n",
    "\n",
    "def sin_transform(values):\n",
    "    return np.sin(2 * np.pi * values)\n",
    "def cos_transform(values):\n",
    "    return np.cos(2 * np.pi * values)\n",
    "\n",
    "train_df['sin_weekday'] = sin_transform(weekday_periodic)\n",
    "train_df['cos_weekday'] = cos_transform(weekday_periodic)\n",
    "train_df['sin_hour'] = sin_transform(hour_periodic)\n",
    "train_df['cos_hour'] = cos_transform(hour_periodic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_dummy = pd.get_dummies(train_df['month']).rename(columns={6:'month_6', 7:'month_7', 8:'month_8'})\n",
    "train_df = pd.concat([train_df, month_dummy[['month_6', 'month_7']]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add holiday column\n",
    "# holiday = 1, workday = 0\n",
    "train_df['holiday'] = train_df.apply(lambda x : 0 if x['day']<5 else 1, axis = 1)\n",
    "train_df.loc[(train_df.date == datetime.date(2022, 6, 6))&(train_df.date == datetime.date(2022, 8, 15)), 'holiday'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add DI column (DI : Discomfort Index (불쾌지수))\n",
    "train_df['DI'] = 9/5*train_df['temperature'] - 0.55*(1 - train_df['humidity']/100) * (9/5*train_df['humidity'] - 26) + 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add CDH column (CDH : Cooling Degree-Day)\n",
    "# CDD(냉방도일) -> CDH(냉방도시)로 데이터에 맞게 변형한 개념\n",
    "# 냉방도일 : 어느 지방의 더운 정도를 나타내는 지표로 사용 (sum(해당 일의 기온 - 기준온도))\n",
    "# 냉방도일이 크면 연료의 소비량이 많아짐\n",
    "# 실내온도가 같아도 외부 기온은 지역별로 다름 -> 지역마다 값이 다르게 나타남 -> 이 데이터에서는 건물별로 다르다고 얘기할 수 있음\n",
    "# 냉방도시 : sum(해당 시간의 기온 - 기준온도)\n",
    "def CDH(df, num_building):\n",
    "    df_ = df.copy()\n",
    "    cdhs = np.array([])\n",
    "    for num in range(1, num_building+1, 1):\n",
    "        cdh = []\n",
    "        cdh_df = df_[df_['building_num'] == num_building]\n",
    "        cdh_temp = cdh_df['temperature'].values # Series로도 돌릴 수 있지만 array로 돌리는게 속도가 훨씬 빠름\n",
    "        for i in range(len(cdh_temp)):\n",
    "            if i < 11:\n",
    "                cdh.append(np.sum(cdh_temp[:(i+1)] - 26))\n",
    "            else:\n",
    "                cdh.append(np.sum(cdh_temp[(i-11):(i+1)] - 26))\n",
    "        \n",
    "        cdhs = np.concatenate([cdhs, cdh])\n",
    "    \n",
    "    return cdhs\n",
    "\n",
    "train_df['CDH'] = CDH(train_df, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204000/204000 [02:09<00:00, 1571.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# 건물별, 요일별, 시간별 발전량 평균 넣어주기\n",
    "power_mean = pd.pivot_table(train_df, values = 'power_consumption', index = ['building_num', 'hour', 'weekday'], aggfunc = np.mean).reset_index()\n",
    "tqdm.pandas()\n",
    "train_df['day_hour_mean'] = train_df.progress_apply(lambda x : power_mean.loc[(power_mean.building_num == x['building_num']) & (power_mean.hour == x['hour']) & (power_mean.weekday == x['weekday']), 'power_consumption'].values[0], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204000/204000 [01:44<00:00, 1957.30it/s]\n"
     ]
    }
   ],
   "source": [
    "# 건물별, 요일별, 시간별 발전량 표준편차 넣어주기\n",
    "power_std = pd.pivot_table(train_df, values = 'power_consumption', index = ['building_num', 'hour', 'weekday'], aggfunc = np.std).reset_index()\n",
    "tqdm.pandas()\n",
    "train_df['day_hour_std'] = train_df.progress_apply(lambda x : power_std.loc[(power_std.building_num == x['building_num']) & (power_std.hour == x['hour']) & (power_std.weekday == x['weekday']), 'power_consumption'].values[0], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204000/204000 [00:51<00:00, 3979.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# 건물별 시간별 발전량 평균 넣어주기\n",
    "power_hour_mean = pd.pivot_table(train_df, values = 'power_consumption', index = ['building_num', 'hour'], aggfunc = np.mean).reset_index()\n",
    "tqdm.pandas()\n",
    "train_df['hour_mean'] = train_df.progress_apply(lambda x : power_hour_mean.loc[(power_hour_mean.building_num == x['building_num']) & (power_hour_mean.hour == x['hour']) ,'power_consumption'].values[0], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204000/204000 [00:48<00:00, 4224.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# 건물별 시간별 발전량 표준편차 넣어주기\n",
    "power_hour_std = pd.pivot_table(train_df, values = 'power_consumption', index = ['building_num', 'hour'], aggfunc = np.std).reset_index()\n",
    "tqdm.pandas()\n",
    "train_df['hour_std'] = train_df.progress_apply(lambda x : power_hour_std.loc[(power_hour_std.building_num == x['building_num']) & (power_hour_std.hour == x['hour']) ,'power_consumption'].values[0], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df2 = train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df2.drop(columns=['month', 'day', 'weekday', 'hour'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df2['time_idx'] = (train_df2.loc[:, 'date'] - train_df2.iloc[0, 1]).astype('timedelta64[h]').astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_num</th>\n",
       "      <th>date</th>\n",
       "      <th>temperature</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>humidity</th>\n",
       "      <th>power_consumption</th>\n",
       "      <th>sin_weekday</th>\n",
       "      <th>cos_weekday</th>\n",
       "      <th>sin_hour</th>\n",
       "      <th>...</th>\n",
       "      <th>month_6</th>\n",
       "      <th>month_7</th>\n",
       "      <th>holiday</th>\n",
       "      <th>DI</th>\n",
       "      <th>CDH</th>\n",
       "      <th>day_hour_mean</th>\n",
       "      <th>day_hour_std</th>\n",
       "      <th>hour_mean</th>\n",
       "      <th>hour_std</th>\n",
       "      <th>time_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-01 00:00:00</td>\n",
       "      <td>18.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1085.28</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>49.6576</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>1774.744615</td>\n",
       "      <td>517.982222</td>\n",
       "      <td>1706.318118</td>\n",
       "      <td>446.882767</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-01 01:00:00</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1047.36</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.269797</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47.7625</td>\n",
       "      <td>-22.8</td>\n",
       "      <td>1687.347692</td>\n",
       "      <td>500.769931</td>\n",
       "      <td>1622.620235</td>\n",
       "      <td>439.662704</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-01 02:00:00</td>\n",
       "      <td>17.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>45.0</td>\n",
       "      <td>974.88</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.519584</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47.2225</td>\n",
       "      <td>-35.3</td>\n",
       "      <td>1571.483077</td>\n",
       "      <td>465.227458</td>\n",
       "      <td>1506.971294</td>\n",
       "      <td>412.071906</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-01 03:00:00</td>\n",
       "      <td>16.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>48.0</td>\n",
       "      <td>953.76</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.730836</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44.7856</td>\n",
       "      <td>-47.9</td>\n",
       "      <td>1522.153846</td>\n",
       "      <td>436.601091</td>\n",
       "      <td>1437.365647</td>\n",
       "      <td>391.205981</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-01 04:00:00</td>\n",
       "      <td>18.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>43.0</td>\n",
       "      <td>986.40</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.887885</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>49.0061</td>\n",
       "      <td>-60.1</td>\n",
       "      <td>1506.793846</td>\n",
       "      <td>405.518091</td>\n",
       "      <td>1447.321412</td>\n",
       "      <td>381.099697</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   building_num                date  temperature  precipitation  windspeed  \\\n",
       "0             1 2022-06-01 00:00:00         18.6            0.0        0.9   \n",
       "1             1 2022-06-01 01:00:00         18.0            0.0        1.1   \n",
       "2             1 2022-06-01 02:00:00         17.7            0.0        1.5   \n",
       "3             1 2022-06-01 03:00:00         16.7            0.0        1.4   \n",
       "4             1 2022-06-01 04:00:00         18.4            0.0        2.8   \n",
       "\n",
       "   humidity  power_consumption  sin_weekday  cos_weekday  sin_hour  ...  \\\n",
       "0      42.0            1085.28     0.866025         -0.5  0.000000  ...   \n",
       "1      45.0            1047.36     0.866025         -0.5  0.269797  ...   \n",
       "2      45.0             974.88     0.866025         -0.5  0.519584  ...   \n",
       "3      48.0             953.76     0.866025         -0.5  0.730836  ...   \n",
       "4      43.0             986.40     0.866025         -0.5  0.887885  ...   \n",
       "\n",
       "   month_6  month_7  holiday       DI   CDH  day_hour_mean  day_hour_std  \\\n",
       "0        1        0        0  49.6576 -11.0    1774.744615    517.982222   \n",
       "1        1        0        0  47.7625 -22.8    1687.347692    500.769931   \n",
       "2        1        0        0  47.2225 -35.3    1571.483077    465.227458   \n",
       "3        1        0        0  44.7856 -47.9    1522.153846    436.601091   \n",
       "4        1        0        0  49.0061 -60.1    1506.793846    405.518091   \n",
       "\n",
       "     hour_mean    hour_std  time_idx  \n",
       "0  1706.318118  446.882767         0  \n",
       "1  1622.620235  439.662704         1  \n",
       "2  1506.971294  412.071906         2  \n",
       "3  1437.365647  391.205981         3  \n",
       "4  1447.321412  381.099697         4  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the missing values of precipitation\n",
    "test_df['precipitation'].fillna(0, inplace=True) # 강수량은 0으로 채움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the missing values of wind speed and humidity\n",
    "def fill_missing_with_avg(df, columns):\n",
    "    for i in range(len(df)):\n",
    "        if pd.isna(df.loc[i, columns]):\n",
    "            \n",
    "            prev_value_sum = df.loc[i-4:i-1, columns].sum()\n",
    "            next_value_sum = df.loc[i+1:i+4, columns].sum()\n",
    "            avg_value = (prev_value_sum + next_value_sum) / 8\n",
    "\n",
    "            df.loc[i, columns] = avg_value\n",
    "\n",
    "fill_missing_with_avg(test_df, 'windspeed')\n",
    "fill_missing_with_avg(test_df, 'humidity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the type of date from object to datetime\n",
    "test_df['date'] = pd.to_datetime(test_df['date'], format='%Y%m%d %H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decomposition of time elements\n",
    "test_df['month'] = test_df.date.dt.month\n",
    "test_df['day'] = test_df.date.dt.day\n",
    "test_df['weekday'] = test_df.date.dt.weekday\n",
    "test_df['hour'] = test_df.date.dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add time periodic columns\n",
    "day_periodic = (test_df['day'] - 1) / 30 # 1부터 시작하므로 1을 빼줌\n",
    "weekday_periodic = test_df['weekday'] / 6\n",
    "hour_periodic = test_df['hour'] / 23\n",
    "\n",
    "def sin_transform(values):\n",
    "    return np.sin(2 * np.pi * values)\n",
    "def cos_transform(values):\n",
    "    return np.cos(2 * np.pi * values)\n",
    "\n",
    "test_df['sin_weekday'] = sin_transform(weekday_periodic)\n",
    "test_df['cos_weekday'] = cos_transform(weekday_periodic)\n",
    "test_df['sin_hour'] = sin_transform(hour_periodic)\n",
    "test_df['cos_hour'] = cos_transform(hour_periodic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['month_6'] = 0\n",
    "test_df['month_7'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add holiday column\n",
    "# holiday = 1, workday = 0\n",
    "test_df['holiday'] = test_df.apply(lambda x : 0 if x['day']<5 else 1, axis = 1)\n",
    "test_df.loc[(test_df.date == datetime.date(2022, 6, 6))&(test_df.date == datetime.date(2022, 8, 15)), 'holiday'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add DI column (DI : Discomfort Index (불쾌지수))\n",
    "test_df['DI'] = 9/5*test_df['temperature'] - 0.55*(1 - test_df['humidity']/100) * (9/5*test_df['humidity'] - 26) + 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add CDH column (CDH : Cooling Degree-Day)\n",
    "# CDD(냉방도일) -> CDH(냉방도시)로 데이터에 맞게 변형한 개념\n",
    "# 냉방도일 : 어느 지방의 더운 정도를 나타내는 지표로 사용 (sum(해당 일의 기온 - 기준온도))\n",
    "# 냉방도일이 크면 연료의 소비량이 많아짐\n",
    "# 실내온도가 같아도 외부 기온은 지역별로 다름 -> 지역마다 값이 다르게 나타남 -> 이 데이터에서는 건물별로 다르다고 얘기할 수 있음\n",
    "# 냉방도시 : sum(해당 시간의 기온 - 기준온도)\n",
    "def CDH(df, num_building):\n",
    "    df_ = df.copy()\n",
    "    cdhs = np.array([])\n",
    "    for num in range(1, num_building+1, 1):\n",
    "        cdh = []\n",
    "        cdh_df = df_[df_['building_num'] == num_building]\n",
    "        cdh_temp = cdh_df['temperature'].values # Series로도 돌릴 수 있지만 array로 돌리는게 속도가 훨씬 빠름\n",
    "        for i in range(len(cdh_temp)):\n",
    "            if i < 11:\n",
    "                cdh.append(np.sum(cdh_temp[:(i+1)] - 26))\n",
    "            else:\n",
    "                cdh.append(np.sum(cdh_temp[(i-11):(i+1)] - 26))\n",
    "        \n",
    "        cdhs = np.concatenate([cdhs, cdh])\n",
    "    \n",
    "    return cdhs\n",
    "\n",
    "test_df['CDH'] = CDH(test_df, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16800/16800 [00:06<00:00, 2585.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# 건물별, 요일별, 시간별 발전량 평균 넣어주기\n",
    "tqdm.pandas()\n",
    "test_df['day_hour_mean'] = test_df.progress_apply(lambda x : power_mean.loc[(power_mean.building_num == x['building_num']) & (power_mean.hour == x['hour']) & (power_mean.weekday == x['weekday']), 'power_consumption'].values[0], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16800/16800 [00:06<00:00, 2446.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# 건물별, 요일별, 시간별 발전량 표준편차 넣어주기\n",
    "tqdm.pandas()\n",
    "test_df['day_hour_std'] = test_df.progress_apply(lambda x : power_std.loc[(power_std.building_num == x['building_num']) & (power_std.hour == x['hour']) & (power_std.weekday == x['weekday']), 'power_consumption'].values[0], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16800/16800 [00:04<00:00, 4080.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# 건물별 시간별 발전량 평균 넣어주기\n",
    "tqdm.pandas()\n",
    "test_df['hour_mean'] = test_df.progress_apply(lambda x : power_hour_mean.loc[(power_hour_mean.building_num == x['building_num']) & (power_hour_mean.hour == x['hour']) ,'power_consumption'].values[0], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16800/16800 [00:04<00:00, 3867.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# 건물별 시간별 발전량 표준편차 넣어주기\n",
    "tqdm.pandas()\n",
    "test_df['hour_std'] = test_df.progress_apply(lambda x : power_hour_std.loc[(power_hour_std.building_num == x['building_num']) & (power_hour_std.hour == x['hour']) ,'power_consumption'].values[0], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df2 = test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df2.drop(columns=['month', 'day', 'weekday', 'hour'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df2['time_idx'] = (test_df2.loc[:, 'date'] - train_df2.iloc[0, 1]).astype('timedelta64[h]').astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_num</th>\n",
       "      <th>date</th>\n",
       "      <th>temperature</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>humidity</th>\n",
       "      <th>sin_weekday</th>\n",
       "      <th>cos_weekday</th>\n",
       "      <th>sin_hour</th>\n",
       "      <th>cos_hour</th>\n",
       "      <th>month_6</th>\n",
       "      <th>month_7</th>\n",
       "      <th>holiday</th>\n",
       "      <th>DI</th>\n",
       "      <th>CDH</th>\n",
       "      <th>day_hour_mean</th>\n",
       "      <th>day_hour_std</th>\n",
       "      <th>hour_mean</th>\n",
       "      <th>hour_std</th>\n",
       "      <th>time_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-08-25 00:00:00</td>\n",
       "      <td>23.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>72</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>58.3456</td>\n",
       "      <td>-5.8</td>\n",
       "      <td>1627.80</td>\n",
       "      <td>446.984913</td>\n",
       "      <td>1706.318118</td>\n",
       "      <td>446.882767</td>\n",
       "      <td>2040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-08-25 01:00:00</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>72</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.269797</td>\n",
       "      <td>0.962917</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>57.4456</td>\n",
       "      <td>-12.1</td>\n",
       "      <td>1550.08</td>\n",
       "      <td>449.091398</td>\n",
       "      <td>1622.620235</td>\n",
       "      <td>439.662704</td>\n",
       "      <td>2041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-08-25 02:00:00</td>\n",
       "      <td>22.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>75</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.519584</td>\n",
       "      <td>0.854419</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>57.8725</td>\n",
       "      <td>-18.7</td>\n",
       "      <td>1431.12</td>\n",
       "      <td>415.453568</td>\n",
       "      <td>1506.971294</td>\n",
       "      <td>412.071906</td>\n",
       "      <td>2042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-08-25 03:00:00</td>\n",
       "      <td>22.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>78</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.730836</td>\n",
       "      <td>0.682553</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>57.9376</td>\n",
       "      <td>-25.5</td>\n",
       "      <td>1372.20</td>\n",
       "      <td>378.117772</td>\n",
       "      <td>1437.365647</td>\n",
       "      <td>391.205981</td>\n",
       "      <td>2043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-08-25 04:00:00</td>\n",
       "      <td>21.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>77</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.887885</td>\n",
       "      <td>0.460065</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>56.9961</td>\n",
       "      <td>-32.5</td>\n",
       "      <td>1381.72</td>\n",
       "      <td>360.348483</td>\n",
       "      <td>1447.321412</td>\n",
       "      <td>381.099697</td>\n",
       "      <td>2044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   building_num                date  temperature  precipitation  windspeed  \\\n",
       "0             1 2022-08-25 00:00:00         23.5            0.0        2.2   \n",
       "1             1 2022-08-25 01:00:00         23.0            0.0        0.9   \n",
       "2             1 2022-08-25 02:00:00         22.7            0.0        1.5   \n",
       "3             1 2022-08-25 03:00:00         22.1            0.0        1.3   \n",
       "4             1 2022-08-25 04:00:00         21.8            0.0        1.0   \n",
       "\n",
       "   humidity   sin_weekday  cos_weekday  sin_hour  cos_hour  month_6  month_7  \\\n",
       "0        72  1.224647e-16         -1.0  0.000000  1.000000        0        0   \n",
       "1        72  1.224647e-16         -1.0  0.269797  0.962917        0        0   \n",
       "2        75  1.224647e-16         -1.0  0.519584  0.854419        0        0   \n",
       "3        78  1.224647e-16         -1.0  0.730836  0.682553        0        0   \n",
       "4        77  1.224647e-16         -1.0  0.887885  0.460065        0        0   \n",
       "\n",
       "   holiday       DI   CDH  day_hour_mean  day_hour_std    hour_mean  \\\n",
       "0        1  58.3456  -5.8        1627.80    446.984913  1706.318118   \n",
       "1        1  57.4456 -12.1        1550.08    449.091398  1622.620235   \n",
       "2        1  57.8725 -18.7        1431.12    415.453568  1506.971294   \n",
       "3        1  57.9376 -25.5        1372.20    378.117772  1437.365647   \n",
       "4        1  56.9961 -32.5        1381.72    360.348483  1447.321412   \n",
       "\n",
       "     hour_std  time_idx  \n",
       "0  446.882767      2040  \n",
       "1  439.662704      2041  \n",
       "2  412.071906      2042  \n",
       "3  391.205981      2043  \n",
       "4  381.099697      2044  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transform the dataset for TFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = ['building_num', 'month_6', 'month_7', 'holiday']\n",
    "\n",
    "for col in categorical:\n",
    "        train_df2[col] = train_df2[col].astype(str).astype('category')\n",
    "\n",
    "for col in categorical:\n",
    "        test_df2[col] = test_df2[col].astype(str).astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df2['power_consumption'] = np.nan\n",
    "test_df2 = test_df2[test_df2.columns[0:6].to_list() + test_df2.columns[-1:].to_list() + test_df2.columns[7:-1].to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['building_num', 'date', 'temperature', 'precipitation', 'windspeed',\n",
       "       'humidity', 'power_consumption', 'cos_weekday', 'sin_hour', 'cos_hour',\n",
       "       'month_6', 'month_7', 'holiday', 'DI', 'CDH', 'day_hour_mean',\n",
       "       'day_hour_std', 'hour_mean', 'hour_std', 'time_idx'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df2['day_hour_mean'] = np.nan\n",
    "test_df2['day_hour_std'] = np.nan\n",
    "test_df2['hour_mean'] = np.nan\n",
    "test_df2['hour_std'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(train_df, validate=False):\n",
    "    max_encoder_length = 24 * 7 * encoder_length_in_weeks\n",
    "    max_prediction_length = 24 * 7\n",
    "    training_cutoff = train_df['time_idx'].max() - max_prediction_length # 2039 - 24*7 = 1871\n",
    "\n",
    "    tr_ds = TimeSeriesDataSet(\n",
    "      train_df[lambda x: x.time_idx <=training_cutoff] if validate else train_df, \n",
    "      time_idx = \"time_idx\",\n",
    "      target = \"power_consumption\",\n",
    "      group_ids=[\"building_num\"],\n",
    "      min_encoder_length = 1,\n",
    "      max_encoder_length = max_encoder_length, \n",
    "      min_prediction_length=1, \n",
    "      max_prediction_length=max_prediction_length,\n",
    "\n",
    "      # Known Inputs 알고 있는 변수\n",
    "      time_varying_known_categoricals = categorical, \n",
    "      time_varying_known_reals=[\n",
    "            \"time_idx\",\n",
    "            \"temperature\",\n",
    "            \"windspeed\",\n",
    "            \"humidity\",\n",
    "            \"precipitation\",\n",
    "            'sin_weekday',\n",
    "            'cos_weekday',\n",
    "            'sin_hour', \n",
    "            'cos_hour',\n",
    "            'DI',\n",
    "            'CDH'\n",
    "        ],\n",
    "      target_normalizer=GroupNormalizer(groups=[\"building_num\"], transformation=\"softplus\"),\n",
    "      \n",
    "      # Future Inputs\n",
    "      time_varying_unknown_categoricals=[],\n",
    "      time_varying_unknown_reals=[\n",
    "            \"power_consumption\",\n",
    "            \"day_hour_mean\",\n",
    "            \"day_hour_std\",\n",
    "            \"hour_mean\",\n",
    "            \"hour_std\",\n",
    "        ],\n",
    "\n",
    "        \n",
    "        add_relative_time_idx=True,  # add as feature\n",
    "        add_target_scales=True,  # add as feature\n",
    "        add_encoder_length=True,  # add as feature\n",
    "    )\n",
    "  \n",
    "\n",
    "    va_ds = None\n",
    "    if validate:\n",
    "        va_ds = TimeSeriesDataSet.from_dataset(\n",
    "        tr_ds, train_df, predict=True, stop_randomization=True\n",
    "    )\n",
    "\n",
    "    return tr_ds, va_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_ds, va_ds = load_dataset(train_df2, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training\n",
    "# def fit(seed, tr_ds, va_loader=None):\n",
    "#     seed_everything(seed)\n",
    "\n",
    "#     # create dataloaders for model\n",
    "#     tr_loader = tr_ds.to_dataloader(\n",
    "#         train=True, batch_size=batch_size, num_workers=12\n",
    "#     )\n",
    "\n",
    "#     if va_loader is not None:\n",
    "#         # stop training, when loss metric does not improve on validation set\n",
    "#         early_stopping_callback = EarlyStopping(\n",
    "#             monitor=\"val_loss\",\n",
    "#             min_delta=1e-4,\n",
    "#             patience=20,\n",
    "#             verbose=True,\n",
    "#             mode=\"min\"\n",
    "#         )\n",
    "#         lr_logger = LearningRateMonitor(logging_interval=\"epoch\")  # log the learning rate\n",
    "#         callbacks = [lr_logger, early_stopping_callback]\n",
    "#     else:\n",
    "#         # gather 10 checkpoints with best traing loss\n",
    "#         checkpoint_callback = ModelCheckpoint(\n",
    "#             monitor='train_loss',\n",
    "#             dirpath='tft/',\n",
    "#             filename=f'seed={seed}'+'-{epoch:03d}-{train_loss:.2f}',\n",
    "#             save_top_k=10\n",
    "#         )\n",
    "#         callbacks = [checkpoint_callback]\n",
    "\n",
    "#     # create trainer\n",
    "#     trainer = pl.Trainer(\n",
    "#         max_epochs=66,\n",
    "#         gpus=-1,\n",
    "#         gradient_clip_val=params['gradient_clip_val'],\n",
    "#         limit_train_batches=30,\n",
    "#         callbacks=callbacks,\n",
    "#         logger=TensorBoardLogger('tft_logs', name='my_model')\n",
    "#     )\n",
    "\n",
    "#     # use pre-deterined leraning rate schedule for final submission\n",
    "#     learning_rate = lrs if va_loader is None else params['learning_rate']\n",
    "\n",
    "#     # initialise model with pre-determined hyperparameters\n",
    "#     tft = TemporalFusionTransformer.from_dataset(\n",
    "#         tr_ds,\n",
    "#         learning_rate=learning_rate,\n",
    "#         hidden_size=params['hidden_size'],\n",
    "#         attention_head_size=params['attention_head_size'],\n",
    "#         dropout=params['dropout'],\n",
    "#         hidden_continuous_size=params['hidden_continuous_size'],\n",
    "#         output_size=1,\n",
    "#         loss=SMAPE(), # SMAPE loss\n",
    "#         log_interval=10,  # log example every 10 batches\n",
    "#         logging_metrics=[SMAPE()],\n",
    "#         reduce_on_plateau_patience=4  # reduce learning automatically\n",
    "#     )\n",
    "#     print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "\n",
    "#     kwargs = {'train_dataloader': tr_loader}\n",
    "#     if va_loader:\n",
    "#         kwargs['val_dataloaders'] = va_loader\n",
    "\n",
    "#     # fit network\n",
    "#     trainer.fit(\n",
    "#         tft,\n",
    "#         **kwargs\n",
    "#     )\n",
    "\n",
    "#     best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "#     print(f'best_model_path={best_model_path}')\n",
    "#     best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "\n",
    "#     return best_tft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kijeong\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\dataloader.py:487: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "seed_everything(seed)\n",
    "\n",
    "va_loader = None\n",
    "\n",
    "# create dataloaders for model\n",
    "tr_loader = tr_ds.to_dataloader(\n",
    "    train=True, batch_size=batch_size, num_workers=6\n",
    ")\n",
    "\n",
    "if va_loader is not None:\n",
    "    # stop training, when loss metric does not improve on validation set\n",
    "    early_stopping_callback = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        min_delta=1e-4,\n",
    "        patience=20,\n",
    "        verbose=True,\n",
    "        mode=\"min\"\n",
    "    )\n",
    "    lr_logger = LearningRateMonitor(logging_interval=\"epoch\")  # log the learning rate\n",
    "    callbacks = [lr_logger, early_stopping_callback]\n",
    "else:\n",
    "    # gather 10 checkpoints with best traing loss\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='train_loss',\n",
    "        dirpath='tft/',\n",
    "        filename=f'seed={seed}'+'-{epoch:03d}-{train_loss:.2f}',\n",
    "        save_top_k=10\n",
    "    )\n",
    "    callbacks = [checkpoint_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kijeong\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=-1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=-1)` instead.\n",
      "  rank_zero_deprecation(\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# create trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator='gpu'\n",
    "    devices=-1\n",
    "    gradient_clip_val=params['gradient_clip_val'],\n",
    "    limit_train_batches=30,\n",
    "    callbacks=callbacks,\n",
    "    logger=TensorBoardLogger('tft_logs', name='my_model')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pre-deterined leraning rate schedule for final submission\n",
    "learning_rate = lrs if va_loader is None else params['learning_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 57.7k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kijeong\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "# initialise model with pre-determined hyperparameters\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    tr_ds,\n",
    "    learning_rate=learning_rate,\n",
    "    hidden_size=params['hidden_size'],\n",
    "    attention_head_size=params['attention_head_size'],\n",
    "    dropout=params['dropout'],\n",
    "    hidden_continuous_size=params['hidden_continuous_size'],\n",
    "    output_size=1,\n",
    "    loss=SMAPE(), # SMAPE loss\n",
    "    log_interval=10,  # log example every 10 batches\n",
    "    logging_metrics=[SMAPE()],\n",
    "    reduce_on_plateau_patience=4  # reduce learning automatically\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'train_dataloader': tr_loader}\n",
    "if va_loader:\n",
    "    kwargs['val_dataloaders'] = va_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kijeong\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 2.0 K \n",
      "3  | prescalers                         | ModuleDict                      | 400   \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 2.5 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 19.2 K\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 13.1 K\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.7 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.7 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.7 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.7 K \n",
      "11 | lstm_encoder                       | LSTM                            | 3.4 K \n",
      "12 | lstm_decoder                       | LSTM                            | 3.4 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 840   \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 40    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 2.1 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.0 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 880   \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.7 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 880   \n",
      "20 | output_layer                       | Linear                          | 21    \n",
      "----------------------------------------------------------------------------------------\n",
      "57.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "57.7 K    Total params\n",
      "0.231     Total estimated model params size (MB)\n",
      "C:\\Users\\kijeong\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (30) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/30 [05:03<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/30 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kijeong\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\dataloader.py:487: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "# fit network\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    kwargs\n",
    ")\n",
    "\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "print(f'best_model_path={best_model_path}')\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "# 20분 돌려도 안 돌아감.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict 1 week\n",
    "def forecast(ckpt, train_df, test_df):\n",
    "    # load model\n",
    "    best_tft = TemporalFusionTransformer.load_from_checkpoint(ckpt)\n",
    "    max_encoder_length = best_tft.dataset_parameters['max_encoder_length']\n",
    "    max_prediction_length = best_tft.dataset_parameters['max_prediction_length']\n",
    "\n",
    "    assert max_encoder_length == 5*24*7 and max_prediction_length == 1*24*7\n",
    "\n",
    "    # use 5 weeks of training data at the end\n",
    "    encoder_data = train_df[lambda x: x.time_idx > x.time_idx.max() - max_encoder_length]\n",
    "\n",
    "    # get last entry from training data\n",
    "    last_data = train_df.iloc[[-1]]\n",
    "\n",
    "    # fill NA target value in test data with last values from the train dataset\n",
    "    target_cols = [c for c in test_df.columns if 'target' in c]\n",
    "    for c in target_cols:\n",
    "        test_df.loc[:, c] = last_data[c].item()\n",
    "\n",
    "    decoder_data = test_df\n",
    "\n",
    "    # combine encoder and decoder data. decoder data is to be predicted\n",
    "    new_prediction_data = pd.concat([encoder_data, decoder_data], ignore_index=True)\n",
    "    new_raw_predictions, new_x = best_tft.predict(new_prediction_data, mode=\"raw\", return_x=True)\n",
    "\n",
    "    # num_labels: mapping from 'num' categorical feature to index in new_raw_predictions['prediction']\n",
    "    #             {'5': 4, '6': 6, ...}\n",
    "    # new_raw_predictions['prediction'].shape = (60, 168, 1)\n",
    "    num_labels = best_tft.dataset_parameters['categorical_encoders']['num'].classes_\n",
    "\n",
    "    preds = new_raw_predictions['prediction'].squeeze()\n",
    "\n",
    "    sub_df = pd.read_csv(../data/sample_submission.csv\")\n",
    "\n",
    "    # get prediction for each building (num)\n",
    "    for n, ix in num_labels.items():\n",
    "        sub_df.loc[sub_df.num_date_time.str.startswith(f\"{n} \"), 'answer'] = preds[ix].numpy()\n",
    "\n",
    "    # save predction to a csv file\n",
    "    outfn = CSVROOT+'/'+(Path(ckpt).stem + '.csv')\n",
    "    print(outfn)\n",
    "    sub_df.to_csv(outfn, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble(outfn):\n",
    "    # get all prediction csv files\n",
    "    fns = list(glob.glob(CSVROOT+\"/*.csv\"))\n",
    "    df0 = pd.read_csv(fns[0])\n",
    "    df = pd.concat([df0] + [pd.read_csv(fn).loc[:,'answer'] for fn in fns[1:]], axis=1)\n",
    "    # get median of all predcitions\n",
    "    df['median'] = df.iloc[:,1:].median(axis=1)\n",
    "    df = df[['num_date_time', 'median']]\n",
    "    df = df.rename({'median': 'answer'}, axis=1)\n",
    "    # save to submission file\n",
    "    df.to_csv(outfn, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "print(\"### FORECAST ###\")\n",
    "for p in glob.glob(CKPTROOT + \"/*.ckpt\"):\n",
    "    forecast(p, train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### ENSEMBLING ###\")\n",
    "ensemble(SUBFN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
